import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm


def _validate(model, dataloader):
    model.eval()
    eval_train_correct, eval_train_total = .0, .0
    preds = []
    truths = []
    epoch_stats = {}

    with torch.no_grad():
        for inputs, labels in tqdm(dataloader):
            outputs, _, _ = model(inputs)

            probs = F.softmax(outputs, dim=-1).data[:, 1].numpy().ravel()

            predicted_labels = np.where(probs > 0.5, 1, 0)
            preds.extend(probs)
            truths.extend(labels.numpy().ravel())

            eval_train_total += labels.size(0)
            eval_train_correct += (predicted_labels == labels.numpy().ravel()).sum().item()

    epoch_stats['val_acc'] = eval_train_correct * 1.0 / eval_train_total
    epoch_stats['val_preds'] = {"y_true": np.where(np.array(truths) > 0.5, "malware", "benign"),
                                "y_pred": np.where(np.array(preds) > 0.5, "malware", "benign"),
                                "y_prob": np.array(preds)}

    return epoch_stats


def _train(model, dataloader, optimizer, criterion):
    model.train()
    epoch_loss, epoch_acc, total = .0, .0, .0
    epoch_stats = {}

    for inputs, labels in tqdm(dataloader):
        optimizer.zero_grad()
        outputs, _, _ = model(inputs)
        loss = criterion(outputs, labels)

        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()

        with torch.no_grad():
            probs = F.softmax(outputs, dim=-1).data[:, 1].numpy().ravel()

        predicted_label = np.where(probs > 0.5, 1, 0)
        total += labels.size(0)
        epoch_acc += (predicted_label == labels.numpy().ravel()).sum().item()

    epoch_stats['train_acc'] = epoch_acc * 1.0 / total
    epoch_stats['train_loss'] = epoch_loss * 1.0 / total

    return epoch_stats


def train(model, dataloader, optimizer, criterion, epochs, verbose=False):
    """
    This function trains the provided model on the provided dataloader using the provided optimizer and criterion.
    The training process is repeated for the number of epochs provided.
    If verbose is set to True, the function will print the loss and accuracy of the model on the train set a
    nd accuracy of the model on the validation set after each epoch.

    :param model: (nn.Module) the model to be trained
    :param dataloader: (Dict[str, DataLoader]) a dictionary containing the train and validation dataloaders
    :param optimizer: (Optimizer) the optimizer to be used for training
    :param criterion: (nn.Module) the criterion to be used for calculating the loss
    :param epochs: (int) the number of times the training process should be repeated
    :param verbose: (bool) whether to print the train and validation metrics after each epoch (default: False)
    """
    for epoch in range(epochs):
        train_metrics = _train(model, dataloader['train'], optimizer, criterion)
        val_metrics = _validate(model, dataloader['validation'])

        if verbose:
            print(f"[Epoch {epoch + 1}] (Train) Loss: {train_metrics['train_loss']:.4f} "
                  f"Accuracy: {train_metrics['train_acc']:.4f} | "
                  f"(Validation) Accuracy: {val_metrics['val_acc']:.4f}")


def test(model, dataloader, verbose=True):
    """
    This function tests the model's performance on the provided dataloader and returns the predictions made by the model.
    If verbose is set to True, the function will also print the accuracy of the model on the test set.

    :param model: (nn.Module) the model to be tested
    :param dataloader: (DataLoader) the dataloader containing the test set
    :param verbose: (bool) whether to print the accuracy of the model on the test set (default: True)
    """
    val_metrics = _validate(model, dataloader)
    if verbose:
        print(f"(Test) Accuracy: {val_metrics['val_acc']:.4f}")
    return val_metrics['val_preds']


def load_checkpoints(checkpoint_path, model, optimizer=None, device='cpu', verbose=False):
    """
    This function loads a checkpoint from the specified file path and loads the model and optimizer state dictionaries
    onto the specified device.
    If the optimizer parameter is not None, the optimizers state dictionary is also loaded.
    If verbose is set to True, the function will print the epoch and loss values associated with the checkpoint.

    :param checkpoint_path: (string) the file path of the checkpoint to be loaded
    :param model: (nn.Module) the model to be loaded with the checkpoint's state dictionary
    :param optimizer: (optimizer) the optimizer to be loaded with the checkpoint's state dictionary (optional)
    :param device: (str) the device to load the state dictionaries onto (default: 'cpu')
    :param verbose: (bool) whether to print the epoch and loss values associated with the checkpoint (default: False)
    """
    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if verbose:
        epoch = checkpoint['epoch'] if 'epoch' in checkpoint else 'NA'
        loss = checkpoint['loss'] if 'loss' in checkpoint else 'NA'
        print(f'Values referred to the epoch {epoch} with loss {loss}')
