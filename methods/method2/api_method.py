import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from nltk import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline


def dataset_extraction(dataset_path, split=False, train_size=.7):
    """
    Given the path to the csv file where all the various features are written , these are returned as sets of values

    :param dataset_path: the path to the csv file to be used
    :param split: a boolean value that allows you to derive both benign set and test set
    :param train_size: The percentage of the sample to be allocated to the training set
    :return: A list containing or two values [X,y] or four values [X_train,X_test,y_train,y_test]
    """
    dataset = pd.read_csv(dataset_path).dropna().reset_index(drop=True)
    X = dataset.API_calls
    y = np.array(list(dataset.target_class))
    return train_test_split(X, y, train_size=train_size, random_state=0) if split else [X, y]


class APIAnalyzer:
    def __init__(self, glen):
        self.glen = glen

    def __call__(self, opcodes):
        return ngrams([opcode for opcode in opcodes.split()], self.glen)


def ngram_model(classifier, ngram_length=3):
    """
    Given an input classifier, the same classifier is returned, which, however, first performs preprocessing by computing the ngrams of the input samples

    :param classifier: classifier to be used
    :param ngram_length: the length of the n-grams
    :return: A classifier that performs preprocessing of the input
    """
    pipeline = Pipeline([('vect_tfidf', TfidfVectorizer(sublinear_tf=True, analyzer=APIAnalyzer(ngram_length))),
                         ('clf', classifier)], verbose=True)
    pipeline.__class__.__name__ = f'TfidfVectorizer_{classifier.__class__.__name__}'
    return pipeline


def train(model, X, y):
    """
    Given a model supervised training of the model is performed

    :param model: the model that needs to be trained
    :param X: input samples
    :param y: the labels associated with each input sample
    :return:
    """
    model.fit(X, y)


def test(model, X):
    """
    Given a model and a set of samples, the labels associated with them are predicted

    :param model: the model that will perform the prediction
    :param X: input samples
    :return: the list of labels predicted and probability
    """
    prob = model.predict_proba(X)
    return prob[::, 1], np.where(np.argmax(prob, axis=1) == 1, 'malware', 'benign')
