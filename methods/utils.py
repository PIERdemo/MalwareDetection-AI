import os
from csv import DictWriter
from pathlib import Path

import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, roc_curve, auc


def plot_folder_size(folder_path):
    """
    This function takes a file path as input and plots a histogram of the file sizes within the specified folder.
    The x-axis is limited to values within the dataset, and the y-axis shows the count of files within that size range.
    :param folder_path: The file path of the folder to be analyzed.
    """
    sns.set_style('darkgrid', {'axes.facecolor': ".9"})
    data = folder_path.split("/")[-1]

    sizes = [p.stat().st_size for p in Path(folder_path).rglob('*') if p.is_file()]
    sizes_kb = [s / 1000 for s in sizes]
    lower, upper = np.percentile(sizes_kb, [0, 90])

    plt.xlim(lower, upper)
    sns.histplot(sizes_kb, color='darkred')
    plt.title(f'Distribution of data size ({data})')
    plt.xlabel('File size (KB)')
    plt.ylabel('File count')
    plt.yticks([])
    plt.show()


def append_to_csv(csv_path, row):
    """
    Given a csv file and a new entry for this file, the entry is written to the end of the file, if the file does not
    exist it is also created

    :param csv_path: the path to the csv file, if not present this will be created
    :param row: the new entry, in the format of a dictionary, to be inserted
    :return:
    """
    with open(csv_path, 'a+', newline='') as csv_file:
        dict_writer = DictWriter(csv_file, fieldnames=list(row.keys()))
        if 0 == os.stat(csv_path).st_size:
            dict_writer.writeheader()
        dict_writer.writerow(row)


def evaluate(model, y_true, y_pred, classes=None):
    """
    All performances following the prediction are evaluated

    :param classes: list of classes, if none it takes the classes associated with the model
    :param model: the model that have performed the prediction
    :param y_true: the list of correct labels
    :param y_pred: the list of labels predicted
    :return:
    """
    print(f'[{model.__class__.__name__}] accuracy percentage of: {accuracy_score(y_true, y_pred) * 1e2:.{3}f}%')
    print(f'[{model.__class__.__name__}] {(y_true == y_pred).sum()} exact prediction over {y_true.shape[0]} samples')
    print(f'\n {"=" * 15} Classification report {"=" * 15} \n {(classification_report(y_true, y_pred))}\n {"=" * 55}')

    if classes is None:
        classes = model.classes_

    cm = confusion_matrix(y_true, y_pred, normalize='true')
    cm_display = ConfusionMatrixDisplay(confusion_matrix=np.around(cm, 2), display_labels=classes)
    cm_display.plot(cmap=plt.cm.Oranges, xticks_rotation=90)
    plt.tight_layout()
    plt.title("Confusion Matrix")
    plt.show()


def ROC_curve(y_true, y_prob):
    """
    Plots the Receiver Operating Characteristic (ROC) curve for a binary classification model.
    The function takes in two input arguments:
    :param y_true: a list or array of true labels for the binary classification
    :param y_prob: a list or array of predicted probabilities of the positive class
    """
    y_true = np.where(np.array(y_true) == 'malware', 1, 0)

    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    sns.set_style('darkgrid', {'axes.facecolor': ".9"})
    plt.figure(figsize=(10, 8))
    lw = 2
    plt.plot(fpr, tpr, color='darkorange', lw=lw, label=f'ROC curve [area = {roc_auc:0.2f}]')
    plt.plot([0, 0, 1], [0, 1, 1], color='darkred', lw=lw, label='Ideal curve')
    plt.plot([1, 0], [1, 0], color='black', lw=lw, linestyle='dashed')

    plt.xlim([-0.03, 1.0])
    plt.ylim([0, 1.03])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

    plt.legend(loc='lower right')
    plt.show()
