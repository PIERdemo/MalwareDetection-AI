import seaborn as sns
from tqdm import tqdm
import os
import numpy as np
import matplotlib.pyplot as plt
from secml.array import CArray

# Importing the genetic engine
from secml_malware.attack.blackbox.ga.c_base_genetic_engine import CGeneticAlgorithm
# Importing the class for the Gamma Sections Evasion attack
from secml_malware.attack.blackbox.c_gamma_sections_evasion import CGammaSectionsEvasionProblem


def attack(net, malware_folder, bening_folder, adv_folder,
           how_many=40, population_size=6, penalty_regularizer=1e-2, iterations=10):
    """
    Given a classifier to attack in input, and three samples folders: malware, benign, adversarial, this method produces
    a set of adversarial samples to fool the net passed, that will be stored in the adversarial folder.

    :param net: Neural network to be attacked
    :param malware_folder: Folder containing malware samples
    :param bening_folder: Folder containing benign samples
    :param adv_folder: Output folder to save the generated adversarial examples
    :param how_many: Number of sections to be extracted from all the benign files
    :param population_size: Population size at each round of the genetic algorithm
    :param penalty_regularizer: Penalty term used in the loss function
    :param iterations: Maximum number of iterations for the genetic algorithm
    """


    # Creating empty lists to store the samples, labels, and file names
    X, y, file_names = [], [], []

    # Iterating through the malware samples to be obfuscated
    for file in os.listdir(malware_folder):
        path = os.path.join(malware_folder, file)

        # Opening the file in binary mode
        with open(path, "rb") as file_handle:
            raw = file_handle.read()

        # Converting the file to a numpy array 2D (requested by MalConv)
        x = CArray(np.frombuffer(raw, dtype=np.uint8)).atleast_2d()

        # Getting the original prediction and confidence of the network on the sample
        _, confidence = net.predict(x, True)

        # If the sample is predicted as malware, append the sample and label to the respective lists
        if confidence[0, 1].item() > .5:
            X.append(x)
            conf = confidence[1, 0].item()
            y.append([1 - conf, conf])
            file_names.append(path)

    # Creating the population for section injection
    section_population, _ = CGammaSectionsEvasionProblem.create_section_population_from_folder(
        bening_folder,  # folder of sources to extract the sections
        how_many=how_many,  # number of sections to be extracted from all the files
        sections_to_extract=['.text'])  # sections to be extracted

    # Creating section injection attack
    attack = CGammaSectionsEvasionProblem(section_population,  # section population
                                          net,  # model to be attacked
                                          population_size=population_size,  # population size at each round of algorithm
                                          penalty_regularizer=penalty_regularizer,
                                          iterations=iterations,  # maximum iterations
                                          threshold=0,  # in order to not early stop the algorithm
                                          seed=7)
    # Initializing the genetic algorithm
    engine = CGeneticAlgorithm(attack)

    # Iterating through the samples and labels
    for sample, label, file_name in tqdm(zip(X, y, file_names)):
        # Running the genetic algorithm on the sample
        _, _, adv_ds, _ = engine.run(sample, CArray(label[1]))
        # Getting the adversarial sample
        adv_x = adv_ds.X[0, :]
        # Writing the adversarial sample to file
        engine.write_adv_to_file(adv_x, adv_folder + f'/{file_name}_adv')


def check_attack_score(net, malware_folder, net_score=False):
    """
    This function checks the effectiveness of the attack by evaluating the network's predictions on a
    folder of adversarial samples

    :param net: Neural network that was attacked
    :param malware_folder: Folder containing the adversarial samples
    :param net_score: if true returns the network's score on the adversarial samples, otherwise returns attack's success rate
    :return: Attack's success rate or network's score on the adversarial samples
    """
    # Checking the effectiveness of the attack
    tot_files = len(os.listdir(malware_folder))
    correct = 0

    # Iterating through the adversarial samples
    for file in os.listdir(malware_folder):

        path = os.path.join(malware_folder, file)
        # Opening the file in binary mode
        with open(path, "rb") as file_handle:
            code = file_handle.read()
        # Converting the file to a numpy array
        x = CArray(np.frombuffer(code, dtype=np.uint8)).atleast_2d()
        # Getting the prediction and confidence of the network on the adversarial sample
        _, confidence = net.predict(x, True)

        # If the sample is predicted as malware, attack succeeded
        if confidence[0, 1].item() > .5:
            correct += 1

    return 100.0 * correct / tot_files if not net_score else (1 - 100.0 * correct / tot_files)



def plot_security_evaluation_curve(epsilon, accuracy):
    """
    This function plots a security evaluation curve, which shows the relationship between the power of the attack
    (represented by epsilon) and the accuracy of the attack.

    :param epsilon: The power of the attack, represented as a list of values.
    :param accuracy: The accuracy of the attack, represented as a list of values.
    """
    sns.set_style('darkgrid', {'axes.facecolor': ".9"})


    fig, ax = plt.subplots()
    ax.plot(epsilon, accuracy, '-o')
    ax.set_xscale('log')
    plt.xlabel('Power of the Attack')
    plt.ylabel('Accuracy of the Attack')
    plt.title('Security Evaluation Curve')
    # Show the plot
    plt.show()


